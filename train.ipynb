{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Populating the interactive namespace from numpy and matplotlib\n"
    }
   ],
   "source": [
    "%pylab inline\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from transformers import (CONFIG_NAME, WEIGHTS_NAME,\n",
    "                                                     GPT2Config,\n",
    "                                                     GPT2Model,\n",
    "                                                     load_tf_weights_in_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=50257,\n",
    "    n_ctx=1024,\n",
    "    n_embd=1600,\n",
    "    n_head=25,\n",
    "    n_layer=48\n",
    ")\n",
    "config.to_json_file('generator/gpt2/models/model_v5/config.json')\n",
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Help on class GPT2Config in module transformers.configuration_gpt2:\n\nclass GPT2Config(transformers.configuration_utils.PretrainedConfig)\n |  GPT2Config(vocab_size=50257, n_positions=1024, n_ctx=1024, n_embd=768, n_layer=12, n_head=12, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, **kwargs)\n |  \n |  Configuration class to store the configuration of a `GPT2Model`.\n |  \n |  Args:\n |      vocab_size: Vocabulary size of `inputs_ids` in `GPT2Model` or a configuration json file.\n |      n_positions: Number of positional embeddings.\n |      n_ctx: Size of the causal mask (usually same as n_positions).\n |      n_embd: Dimensionality of the embeddings and hidden states.\n |      n_layer: Number of hidden layers in the Transformer encoder.\n |      n_head: Number of attention heads for each attention layer in\n |          the Transformer encoder.\n |      layer_norm_epsilon: epsilon to use in the layer norm layers\n |      resid_pdrop: The dropout probabilitiy for all fully connected\n |          layers in the embeddings, encoder, and pooler.\n |      attn_pdrop: The dropout ratio for the attention\n |          probabilities.\n |      embd_pdrop: The dropout ratio for the embeddings.\n |      initializer_range: The sttdev of the truncated_normal_initializer for\n |          initializing all weight matrices.\n |  \n |  Method resolution order:\n |      GPT2Config\n |      transformers.configuration_utils.PretrainedConfig\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, vocab_size=50257, n_positions=1024, n_ctx=1024, n_embd=768, n_layer=12, n_head=12, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, **kwargs)\n |      Constructs GPT2Config.\n |      \n |      Args:\n |          vocab_size: Vocabulary size of `inputs_ids` in `GPT2Model` or a configuration json file.\n |          n_positions: Number of positional embeddings.\n |          n_ctx: Size of the causal mask (usually same as n_positions).\n |          n_embd: Dimensionality of the embeddings and hidden states.\n |          n_layer: Number of hidden layers in the Transformer encoder.\n |          n_head: Number of attention heads for each attention layer in\n |              the Transformer encoder.\n |          layer_norm_epsilon: epsilon to use in the layer norm layers\n |          resid_pdrop: The dropout probabilitiy for all fully connected\n |              layers in the embeddings, encoder, and pooler.\n |          attn_pdrop: The dropout ratio for the attention\n |              probabilities.\n |          embd_pdrop: The dropout ratio for the embeddings.\n |          initializer_range: The sttdev of the truncated_normal_initializer for\n |              initializing all weight matrices.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  hidden_size\n |  \n |  max_position_embeddings\n |  \n |  num_attention_heads\n |  \n |  num_hidden_layers\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  pretrained_config_archive_map = {'distilgpt2': 'https://s3.amazonaws.c...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from transformers.configuration_utils.PretrainedConfig:\n |  \n |  __eq__(self, other)\n |      Return self==value.\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  save_pretrained(self, save_directory)\n |      Save a configuration object to the directory `save_directory`, so that it\n |      can be re-loaded using the :func:`~transformers.PretrainedConfig.from_pretrained` class method.\n |  \n |  to_dict(self)\n |      Serializes this instance to a Python dictionary.\n |  \n |  to_json_file(self, json_file_path)\n |      Save this instance to a json file.\n |  \n |  to_json_string(self)\n |      Serializes this instance to a JSON string.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from transformers.configuration_utils.PretrainedConfig:\n |  \n |  from_dict(json_object) from builtins.type\n |      Constructs a `Config` from a Python dictionary of parameters.\n |  \n |  from_json_file(json_file) from builtins.type\n |      Constructs a `Config` from a json file of parameters.\n |  \n |  from_pretrained(pretrained_model_name_or_path, **kwargs) from builtins.type\n |      Instantiate a :class:`~transformers.PretrainedConfig` (or a derived class) from a pre-trained model configuration.\n |      \n |      Parameters:\n |          pretrained_model_name_or_path: either:\n |      \n |              - a string with the `shortcut name` of a pre-trained model configuration to load from cache or download, e.g.: ``bert-base-uncased``.\n |              - a string with the `identifier name` of a pre-trained model configuration that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n |              - a path to a `directory` containing a configuration file saved using the :func:`~transformers.PretrainedConfig.save_pretrained` method, e.g.: ``./my_model_directory/``.\n |              - a path or url to a saved configuration JSON `file`, e.g.: ``./my_model_directory/configuration.json``.\n |      \n |          cache_dir: (`optional`) string:\n |              Path to a directory in which a downloaded pre-trained model\n |              configuration should be cached if the standard cache should not be used.\n |      \n |          kwargs: (`optional`) dict: key/value pairs with which to update the configuration object after loading.\n |      \n |              - The values in kwargs of any keys which are configuration attributes will be used to override the loaded values.\n |              - Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled by the `return_unused_kwargs` keyword parameter.\n |      \n |          force_download: (`optional`) boolean, default False:\n |              Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n |      \n |          resume_download: (`optional`) boolean, default False:\n |              Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n |      \n |          proxies: (`optional`) dict, default None:\n |              A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n |              The proxies are used on each request.\n |      \n |          return_unused_kwargs: (`optional`) bool:\n |      \n |              - If False, then this function returns just the final configuration object.\n |              - If True, then this functions returns a tuple `(config, unused_kwargs)` where `unused_kwargs` is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: ie the part of kwargs which has not been used to update `config` and is otherwise ignored.\n |      \n |      Examples::\n |      \n |          # We can't instantiate directly the base class `PretrainedConfig` so let's show the examples on a\n |          # derived class: BertConfig\n |          config = BertConfig.from_pretrained('bert-base-uncased')    # Download configuration from S3 and cache.\n |          config = BertConfig.from_pretrained('./test/saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n |          config = BertConfig.from_pretrained('./test/saved_model/my_configuration.json')\n |          config = BertConfig.from_pretrained('bert-base-uncased', output_attention=True, foo=False)\n |          assert config.output_attention == True\n |          config, unused_kwargs = BertConfig.from_pretrained('bert-base-uncased', output_attention=True,\n |                                                             foo=False, return_unused_kwargs=True)\n |          assert config.output_attention == True\n |          assert unused_kwargs == {'foo': False}\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from transformers.configuration_utils.PretrainedConfig:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from transformers.configuration_utils.PretrainedConfig:\n |  \n |  __hash__ = None\n\n"
    }
   ],
   "source": [
    "help(GPT2Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}